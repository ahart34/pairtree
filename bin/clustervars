#!/usr/bin/env python3
import argparse
import numpy as np
import json
import multiprocessing

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
import inputparser
import common
import cluster_pairwise
import cluster_linfreq
import clustermaker

def _convert_assignment_to_clustering(Z, vids):
  uniq_Z  = set(list(Z))
  assert uniq_Z == set(range(len(uniq_Z)))
  clusters = [[vids[vidx] for vidx in np.flatnonzero(Z == cidx)] for cidx in sorted(uniq_Z)]
  return clusters

def _make_init_clusters(variants):
  vids = common.extract_vids(variants)
  clusters = [[vid] for vid in vids]
  return clusters

def _select_best(clusterings, llhs):
  best = np.argmax(llhs)
  return clusterings[best]

def _make_unique(clusterings, llhs):
  uniq_clust, idxs = np.unique(clusterings, return_index=True, axis=0)
  uniq_llhs = llhs[idxs]
  return (uniq_clust, uniq_llhs)

def _sort_clusters(clusters, variants):
  vids, V, T, T_prime, omega = inputparser.load_read_counts(variants)
  assert set(vids) == set([vid for clust in clusters for vid in clust])
  vidmap = {vid: idx for idx, vid in enumerate(vids)}

  phi_hat = []
  for C in clusters:
    vidxs = [vidmap[vid] for vid in C]
    phi_hat.append(np.sum(V[vidxs], axis=0) / np.sum(T_prime[vidxs], axis=0))
  phi_hat = np.array(phi_hat)

  K, S = phi_hat.shape
  phi_hat_mean = np.sum(phi_hat, axis=1) / S
  order = np.argsort(-phi_hat_mean)

  sorted_clusters = [clusters[idx] for idx in order]
  return sorted_clusters

def _make_coclust_logprior(prior, S, scale_prior_with_samps=True):
  assert 0 < prior <= 1
  logprior = {'garbage': -np.inf, 'cocluster': np.log(prior)}
  if scale_prior_with_samps:
    logprior['cocluster'] *= S
  return logprior

def _normalize_logconc(logconc, S):
  logconc /= np.log10(np.e)
  logconc *= S
  logconc = np.maximum(-600, logconc)
  logconc = np.minimum(600,  logconc)
  return logconc

def _cluster(model, variants, init_clusters, logconc, coclust_prior, iterations_per_chain, seed, chains, parallel):
  S = len(list(variants.values())[0]['var_reads'])
  logconc = _normalize_logconc(logconc, S)

  if model == 'pairwise':
    logprior = _make_coclust_logprior(coclust_prior, S)
    supervars, clustrel_posterior, _, _, _ = clustermaker.use_pre_existing(
      variants,
      logprior,
      parallel,
      init_clusters,
      [],
    )
    superclusters = clustermaker.make_superclusters(supervars)
    vids, assigns, llhs = cluster_pairwise.cluster(variants, init_clusters, supervars, superclusters, clustrel_posterior, logconc, iterations_per_chain)

  elif model == 'linfreq':
    vids, assigns, llhs = cluster_linfreq.cluster(variants, init_clusters, logconc, iterations_per_chain)

  elif model == 'both':
    raise Exception('Not yet implemented')

  else:
    raise Exception('Unknown --model: %s' % model)

  assigns, llhs = _make_unique(assigns, llhs)
  return (vids, assigns, llhs)

def main():
  parser = argparse.ArgumentParser(
    description='LOL HI THERE',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
  )
  parser.add_argument('--seed', dest='seed', type=int,
    help='Integer seed used for pseudo-random number generator. Running with the same seed on the same inputs will produce exactly the same result.')
  parser.add_argument('--chains', dest='chains', type=int, default=1,
    help='Number of Gibbs sampling chains to run')
  parser.add_argument('--concentration', dest='logconc', type=float, default=-2,
    help='log10(alpha) for Chinese restaurant process. The larger this is, the stronger the preference for more clusters.')
  parser.add_argument('--iterations', type=int, default=100,
    help='Number of Gibbs sampling iterations')
  parser.add_argument('--parallel', dest='parallel', type=int, default=None,
    help='Number of tasks to run in parallel. By default, this is set to the number of CPU cores on the system. On hyperthreaded systems, this will be twice the number of physical CPUs.')
  parser.add_argument('--prior', type=float, default=0.25,
    help='Pairwise coclustering prior probability. Used only for --model=pairwise or --model=both.')
  parser.add_argument('--init-from-existing', action='store_true',
    help='Initialize clusters from those already provided in `in_params_fn`')
  parser.add_argument('--model', choices=('pairwise', 'linfreq', 'both'), default='linfreq',
    help='Clustering model to use')
  parser.add_argument('ssm_fn')
  parser.add_argument('in_params_fn')
  parser.add_argument('out_params_fn')
  args = parser.parse_args()

  if args.seed is not None:
    seed = args.seed
  else:
    # Maximum seed is 2**32 - 1.
    seed = np.random.randint(2**32)
  np.random.seed(seed)

  np.set_printoptions(linewidth=400, precision=3, threshold=sys.maxsize, suppress=True)
  np.seterr(divide='raise', invalid='raise', over='raise')
  parallel = args.parallel if args.parallel is not None else multiprocessing.cpu_count()

  variants = inputparser.load_ssms(args.ssm_fn)
  orig_params = inputparser.load_params(args.in_params_fn)

  garbage = orig_params.get('garbage', [])
  variants = inputparser.remove_garbage(variants, garbage)

  if args.init_from_existing:
    assert 'clusters' in orig_params
    init_clusters = orig_params['clusters']
  else:
    init_clusters = _make_init_clusters(variants)

  vids, assigns, llhs = _cluster(
    args.model,
    variants,
    init_clusters,
    args.logconc,
    args.prior,
    args.iterations,
    args.seed,
    args.chains,
    args.parallel,
  )
  best_assign = _select_best(assigns, llhs)
  clusters = _convert_assignment_to_clustering(best_assign, vids)
  clusters = _sort_clusters(clusters, variants)

  # We expect `samples` will be specified.
  assert 'samples' in orig_params
  params = {
    'clusters': clusters,
    'garbage': garbage,
    'samples': orig_params['samples'],
  }
  with open(args.out_params_fn, 'w') as F:
    json.dump(params, F)

if __name__ == '__main__':
  main()
