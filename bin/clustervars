#!/usr/bin/env python3
import argparse
import numpy as np
from numba import njit, vectorize
import json

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
import inputparser
import common
import util

@njit
def _calc_cweight_pants(V, T_prime, phi_alpha0, phi_beta0, vidx, members):
  alpha_c = phi_alpha0 + np.sum(V[members],                    axis=0)
  beta_c  = phi_beta0  + np.sum(T_prime[members] - V[members], axis=0)

  data_llh = util.beta_binom_logpmf(V[vidx], T_prime[vidx], alpha_c, beta_c)
  membership_llh = np.log(len(members))
  return membership_llh + np.sum(data_llh)

@njit
def _calc_cweight_socks(V, T_prime, phi_alpha0, phi_beta0, vidx, members):
  V_summed = np.sum(V[members], axis=0)
  R_summed = np.sum(T_prime[members], axis=0) - V_summed
  Vi = V[vidx]
  Ti = T_prime[vidx]
  Ri = Ti - Vi

  result  = util.log_N_choose_K(Ti, Vi)
  result += util.lbeta(V_summed + Vi + phi_alpha0, R_summed + Ri + phi_beta0)
  result -= util.lbeta(V_summed + phi_alpha0, R_summed + phi_beta0)

  return np.log(len(members)) + np.sum(result)

@njit
def _calc_llh_pants(V, T_prime, Z, phi_alpha0, phi_beta0, conc):
  uniq_Z  = set(list(Z))
  C = len(uniq_Z)
  #assert uniq_Z == set(range(C))
  N = len(Z)
  cluster_sizes = np.array([np.sum(Z == c) for c in range(C)])
  assert np.sum(cluster_sizes) == N

  llh  = C * np.log(conc)
  llh += np.sum(util.logfactorial(cluster_sizes - 1))
  llh -= np.sum(np.log(conc + np.arange(N)))

  for cidx in range(C):
    members = np.flatnonzero(Z == cidx)
    alpha_c = phi_alpha0 + np.sum(V[members],                    axis=0)
    beta_c  = phi_beta0  + np.sum(T_prime[members] - V[members], axis=0)
    for vidx in members:
      llh += np.sum(util.beta_binom_logpmf(V[vidx], T_prime[vidx], alpha_c, beta_c))
  return llh

@njit
def _calc_llh_socks(V, T_prime, Z, phi_alpha0, phi_beta0, conc):
  uniq_Z  = set(list(Z))
  C = len(uniq_Z)
  #assert uniq_Z == set(range(C))
  N = len(Z)
  cluster_sizes = np.array([np.sum(Z == c) for c in range(C)])
  assert np.sum(cluster_sizes) == N

  llh  = C * np.log(conc)
  llh += np.sum(util.logfactorial(cluster_sizes - 1))
  llh -= np.sum(np.log(conc + np.arange(N)))

  for cidx in range(C):
    members = np.flatnonzero(Z == cidx)
    V_summed = np.sum(V[members], axis=0)
    T_summed = np.sum(T_prime[members], axis=0)
    R_summed = T_summed - V_summed

    binom_coef = util.log_N_choose_K(T_prime[members], V[members])
    llh_c = np.sum(binom_coef, axis=0)
    llh_c += util.lbeta(V_summed + phi_alpha0, R_summed + phi_beta0)
    llh_c -= util.lbeta(phi_alpha0, phi_beta0)

    llh += np.sum(llh_c)

  return llh

@njit
def _calc_new_cluster_weight_pants(V, T_prime, phi_alpha0, phi_beta0, vidx, conc):
  return np.log(conc) + np.sum(util.beta_binom_logpmf(V[vidx], T_prime[vidx], phi_alpha0, phi_beta0))

@njit
def _calc_new_cluster_weight_socks(V, T_prime, phi_alpha0, phi_beta0, vidx, conc):
  Vi = V[vidx]
  Ti = T_prime[vidx]
  Ri = Ti - Vi

  result  = util.log_N_choose_K(Ti, Vi)
  result += util.lbeta(Vi + phi_alpha0, Ri + phi_beta0)
  # This is subtracted from *each element* of `result`, which is important.
  result -= util.lbeta(phi_alpha0, phi_beta0)

  return np.log(conc) + np.sum(result)

@njit
def _compute_cweights_full(V, T_prime, Z, phi_alpha0, phi_beta0, vidx, conc, C):
  mask = np.ones(len(V), dtype=np.bool_)
  mask[vidx] = 0
  llh_excluded = _calc_llh(V[mask], T_prime[mask], Z[mask], phi_alpha0, phi_beta0, conc)

  cweights_full = np.full(C + 1, np.nan)
  Z_prime = np.copy(Z)

  for cidx in range(C + 1):
    Z_prime[vidx] = cidx
    llh_included = _calc_llh(V, T_prime, Z_prime, phi_alpha0, phi_beta0, conc)
    cweights_full[cidx] = llh_included - llh_excluded

  return cweights_full

@njit
def cluster(V, T_prime, omega, conc, iters, check_gibbs_ratio=False):
  # N: number of variants
  # C: number of clusters
  # Z: cluster assignments
  N = len(V)
  C = 1
  Z = np.zeros(N, np.int32)

  # Beta distribution prior for phi
  phi_alpha0 = 1.
  phi_beta0 = 1.

  clusterings = []
  llhs = []

  for I in range(iters):
    for vidx in range(N):
      old_cluster = Z[vidx]
      Z[vidx] = -1
      if not np.any(Z == old_cluster):
        # If `vidx` was the only member, remove this cluster.
        # Do so by moving the last cluster to its index.
        # (These operations are valid even if `highest == old_cluster`.)
        highest = C - 1
        Z[Z == highest] = old_cluster
        C -= 1

      # `cweights`: LLHs of each cluster destination for `vidx`
      # cweights[C] = LLH of adding new cluster
      cweights = np.empty(C + 1)
      # Consider every possible destination.
      for cidx in range(C):
        members = np.flatnonzero(Z == cidx)
        cweights[cidx] = _calc_cweight(V, T_prime, phi_alpha0, phi_beta0, vidx, members)
      # Consider adding a new cluster.
      cweights[C] = _calc_new_cluster_weight(V, T_prime, phi_alpha0, phi_beta0, vidx, conc)
      cweights -= np.log(conc + N - 1)

      if check_gibbs_ratio:
        cweights_full = _compute_cweights_full(V, T_prime, Z, phi_alpha0, phi_beta0, vidx, conc, C)
        assert np.all(util.isclose(cweights, cweights_full))

      cprobs = util.softmax(cweights)
      new_cluster = util.sample_multinom(cprobs)
      Z[vidx] = new_cluster
      if new_cluster == C:
        C += 1

    llh = _calc_llh(V, T_prime, Z, phi_alpha0, phi_beta0, conc)
    clusterings.append(np.copy(Z))
    llhs.append(llh)
    #print(I, C, llh)

  return (clusterings, llhs)

def make_unique(clusterings, llhs):
  uniq_clust, idxs = np.unique(clusterings, return_index=True, axis=0)
  uniq_llhs = llhs[idxs]
  return (uniq_clust, uniq_llhs)

def list_clusters(Z, variants):
  uniq_Z  = set(list(Z))
  assert uniq_Z == set(range(len(uniq_Z)))

  vids = common.extract_vids(variants)
  clusters = [[vids[vidx] for vidx in np.flatnonzero(Z == cidx)] for cidx in sorted(uniq_Z)]
  return clusters

def main():
  parser = argparse.ArgumentParser(
    description='LOL HI THERE',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
  )
  parser.add_argument('--concentration', type=float, default=1e-2,
    help='Alpha for Chinese restaurant process. The larger this is, the stronger the preference for more clusters.')
  parser.add_argument('--iterations', type=int, default=100,
    help='Number of Gibbs sampling iterations')
  parser.add_argument('--model', choices=('pants', 'socks'), default='socks')
  parser.add_argument('ssm_fn')
  parser.add_argument('in_params_fn')
  parser.add_argument('out_params_fn')
  args = parser.parse_args()

  np.set_printoptions(linewidth=400, precision=3, threshold=sys.maxsize, suppress=True)
  np.seterr(divide='raise', invalid='raise', over='raise')

  if args.model == 'socks':
    globals()['_calc_llh'] = _calc_llh_socks
    globals()['_calc_cweight'] = _calc_cweight_socks
    globals()['_calc_new_cluster_weight'] = _calc_new_cluster_weight_socks
  else:
    globals()['_calc_llh'] = _calc_llh_pants
    globals()['_calc_cweight'] = _calc_cweight_pants
    globals()['_calc_new_cluster_weight'] = _calc_new_cluster_weight_pants

  variants = inputparser.load_ssms(args.ssm_fn)
  V, T, T_prime, omega = inputparser.load_read_counts(variants)
  params = inputparser.load_params(args.in_params_fn)

  clusterings, llhs = cluster(
    V,
    T_prime,
    omega,
    args.concentration,
    args.iterations,
    check_gibbs_ratio = False,
  )
  clusterings, llhs = np.array(clusterings), np.array(llhs)
  #clusterings, llhs = make_unique(clusterings, llhs)
  best = np.argmax(llhs)
  clusters = list_clusters(clusterings[best], variants)

  best_posterior = util.softmax(llhs)[best]
  print(
    best,
    llhs[best],
    -llhs[best] / len(clusterings[0]) / np.log(2),
    best_posterior,
    len(clusterings),
    len(clusters),
  )

  params['clusters'] = clusters
  # If user hasn't already specified garbage, write the params with empty
  # garbage, since running Pairtree requires some value for this.
  if 'garabage' not in params:
    params['garbage'] = []
  with open(args.out_params_fn, 'w') as F:
    json.dump(params, F)

if __name__ == '__main__':
  main()
