#!/usr/bin/env python3
import argparse
import numpy as np
from numba import njit
import math
import json

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
import inputparser
import common
import util

from scipy.special import betaln

@njit
def _logfactorial(X):
  result = np.empty(X.shape)
  for idx in range(len(X)):
    result[idx] = math.lgamma(X[idx] + 1)
  return result

@njit
def _log_N_choose_K(N, K):
  return _logfactorial(N) - (_logfactorial(K) + _logfactorial(N - K))

def beta_binom_logpmf(x, n, a, b):
  assert np.all(a > 0) and np.all(b > 0)
  logpx = _log_N_choose_K(n, x) + betaln(x + a, n - x + b) - betaln(a, b)
  return logpx

def _extract_mat(variants, key):
  vids = common.extract_vids(variants)
  vids = common.sort_vids(vids)
  arr = [variants[vid][key] for vid in vids]
  return np.array(arr)

def _calc_cweight(V, T_prime, phi_alpha0, phi_beta0, vidx, members):
  alpha_c = phi_alpha0 + np.sum(V[members],                    axis=0)
  beta_c  = phi_beta0  + np.sum(T_prime[members] - V[members], axis=0)

  data_llh = beta_binom_logpmf(V[vidx], T_prime[vidx], alpha_c, beta_c)
  membership_llh = np.log(len(members))
  return membership_llh + np.sum(data_llh)

def calc_llh(V, T_prime, Z, phi_alpha0, phi_beta0):
  uniq_Z  = set(list(Z))
  assert uniq_Z == set(range(len(uniq_Z)))

  llh = 0
  for cidx in uniq_Z:
    members = np.flatnonzero(Z == cidx)
    alpha_c = phi_alpha0 + np.sum(V[members],                    axis=0)
    beta_c  = phi_beta0  + np.sum(T_prime[members] - V[members], axis=0)
    for vidx in members:
      llh += np.sum(beta_binom_logpmf(V[vidx], T_prime[vidx], alpha_c, beta_c))
  return llh

def cluster(variants, conc, iters):
  V = _extract_mat(variants, 'var_reads')
  T = _extract_mat(variants, 'total_reads')
  omega = _extract_mat(variants, 'omega_v')
  T_prime = np.maximum(V, omega*T)

  # N: number of variants
  # C: number of clusters
  # Z: cluster assignments
  N = len(V)
  C = 1
  Z = np.zeros(N)

  # Beta distribution prior for phi
  phi_alpha0 = 1
  phi_beta0 = 1

  clusterings = []
  llhs = []

  for I in range(iters):
    for vidx in range(N):
      old_cluster = Z[vidx]
      Z[vidx] = -1
      if not np.any(Z == old_cluster):
        # If `vidx` was the only member, remove this cluster.
        # Do so by moving the last cluster to its index.
        # (These operations are valid even if `highest == old_cluster`.)
        highest = C - 1
        Z[Z == highest] = old_cluster
        C -= 1

      # `cweights`: LLHs of each cluster destination for `vidx`
      # cweights[C] = LLH of adding new cluster
      cweights = np.empty(C + 1)
      # Consider every possible destination.
      for cidx in range(C):
        members = np.flatnonzero(Z == cidx)
        cweights[cidx] = _calc_cweight(V, T_prime, phi_alpha0, phi_beta0, vidx, members)
      # Consider adding a new cluster.
      cweights[C] = np.log(conc) + np.sum(beta_binom_logpmf(V[vidx], T_prime[vidx], phi_alpha0, phi_beta0))

      cweights -= np.log(conc + N - 1)
      cprobs = util.softmax(cweights)
      assert np.isclose(1, np.sum(cprobs))
      new_cluster = np.random.choice(len(cprobs), p=cprobs)
      Z[vidx] = new_cluster
      if new_cluster == C:
        C += 1

    llh = calc_llh(V, T_prime, Z, phi_alpha0, phi_beta0)
    clusterings.append(np.copy(Z))
    llhs.append(llh)
    #print(I, C, llh)

  return (
    np.array(clusterings),
    np.array(llhs),
  )

def make_unique(clusterings, llhs):
  uniq_clust, idxs = np.unique(clusterings, return_index=True, axis=0)
  uniq_llhs = llhs[idxs]
  return (uniq_clust, uniq_llhs)

def list_clusters(Z, variants):
  uniq_Z  = set(list(Z))
  assert uniq_Z == set(range(len(uniq_Z)))

  vids = common.extract_vids(variants)
  vids = common.sort_vids(vids)
  clusters = [[vids[vidx] for vidx in np.flatnonzero(Z == cidx)] for cidx in sorted(uniq_Z)]
  return clusters

def main():
  parser = argparse.ArgumentParser(
    description='LOL HI THERE',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
  )
  parser.add_argument('--concentration', type=float, default=1e-2,
    help='Alpha for Chinese restaurant process. The larger this is, the stronger the preference for more clusters.')
  parser.add_argument('--iterations', type=int, default=100,
    help='Number of Gibbs sampling iterations')
  parser.add_argument('ssm_fn')
  parser.add_argument('in_params_fn')
  parser.add_argument('out_params_fn')
  args = parser.parse_args()

  variants = inputparser.load_ssms(args.ssm_fn)
  params = inputparser.load_params(args.in_params_fn)

  clusterings, llhs = cluster(
    variants,
    args.concentration,
    args.iterations,
  )
  #clusterings, llhs = make_unique(clusterings, llhs)
  best = np.argmax(llhs)
  clusters = list_clusters(clusterings[best], variants)

  best_posterior = util.softmax(llhs)[best]
  print(
    best,
    llhs[best],
    -llhs[best] / len(clusterings[0]) / np.log(2),
    best_posterior,
    len(clusterings),
    len(clusters),
  )

  params['clusters'] = clusters
  # If user hasn't already specified garbage, write the params with empty
  # garbage, since running Pairtree requires some value for this.
  if 'garabage' not in params:
    params['garbage'] = []
  with open(args.out_params_fn, 'w') as F:
    json.dump(params, F)

if __name__ == '__main__':
  main()
